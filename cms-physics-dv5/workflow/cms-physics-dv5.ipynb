{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d0a17c-70ee-46fb-aa4a-8ee1441dc0fc",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae86ca0-dc6d-43b5-9c20-253224d7861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "Dataset: ALL\n",
      "ECF range: n=2 to n=3\n"
     ]
    }
   ],
   "source": [
    "# ==================== ANALYSIS CONFIGURATION ====================\n",
    "from pathlib import Path\n",
    "\n",
    "WORKFLOW_DIR = Path.cwd().resolve()\n",
    "\n",
    "\n",
    "# Data Configuration\n",
    "SAMPLES_PATH = WORKFLOW_DIR / \"data\"\n",
    "# SAMPLES_PATH = './data'  # Path to input ROOT files\n",
    "TRIGGERS_FILE = 'triggers.json'  # Path to triggers configuration\n",
    "SAMPLES_READY_FILE = 'samples_ready.json'  # Preprocessed samples metadata\n",
    "\n",
    "# Dataset Selection\n",
    "PROCESS_ALL_DATASETS = True  # Set True to process all datasets, False for specific dataset\n",
    "SUB_DATASET = 'hgg'  # Which dataset to process (e.g., 'hgg', 'hbb', 'qcd', etc.)\n",
    "\n",
    "# Analysis Parameters\n",
    "ECF_UPPER_BOUND = 3  # Calculate ECFs from n=2 to n=ECF_UPPER_BOUND (max: 6)\n",
    "STEP_SIZE = 50_000  # Number of events per chunk for preprocessing\n",
    "\n",
    "# Output Configuration\n",
    "OUTPUT_DIR = 'output'  # Base directory for output parquet files\n",
    "\n",
    "# TaskVine/Dask Configuration\n",
    "# MANAGER_NAME = 'dv5-ecf-manager'  # Name for the TaskVine manager\n",
    "# PORT_RANGE = [9123, 9128]  # Port range for TaskVine manager\n",
    "RUN_INFO_PATH = 'vine-run-info'  # Directory for TaskVine logs\n",
    "STAGING_PATH = './ecf-staging'  # Temporary staging directory\n",
    "\n",
    "# Processing Options\n",
    "PREPROCESS_DATA = True  # Set True to run preprocessing step\n",
    "SHOW_SAMPLES = False  # Set True to display available samples and exit\n",
    "\n",
    "# Advanced TaskVine Tuning (optional)\n",
    "MAX_WORKERS = 30000  # Maximum number of workers\n",
    "MAX_RETRIEVALS = 10  # Max concurrent result retrievals\n",
    "TEMP_REPLICA_COUNT = 1  # Replication count for temp files\n",
    "PRUNE_DEPTH = 0  # Task graph pruning depth\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Dataset: {SUB_DATASET if not PROCESS_ALL_DATASETS else 'ALL'}\")\n",
    "print(f\"ECF range: n=2 to n={ECF_UPPER_BOUND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8181fd4d-fdb3-45b9-95cd-36e5a72bb711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "\n",
    "from coffea import dataset_tools\n",
    "from coffea.nanoevents import PFNanoAODSchema\n",
    "from ndcctools.taskvine.compat import DaskVine\n",
    "\n",
    "# Import helper functions\n",
    "from ecf_helpers import (\n",
    "    preprocess_data,\n",
    "    filter_existing_files,\n",
    "    show_available_samples,\n",
    "    analysis\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", \"Found duplicate branch\")\n",
    "warnings.filterwarnings(\"ignore\", \"Missing cross-reference index for\")\n",
    "warnings.filterwarnings(\"ignore\", \"dcut\")\n",
    "warnings.filterwarnings(\"ignore\", \"Please ensure\")\n",
    "warnings.filterwarnings(\"ignore\", \"invalid value\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e747a-edb9-4cc2-b7ef-1b447880b9b0",
   "metadata": {},
   "source": [
    "# Initialize TaskVine Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1244b22-fb5c-4b76-97b3-ba9fe9dc8d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floability-7f0c88d5-2dea-4717-9dd8-e04cf67058f9\n",
      "Manager Ports: [9123, 9150]\n"
     ]
    }
   ],
   "source": [
    "#manager_name = f\"{os.environ['USER']}-makeDF_2018_mc\";\n",
    "manager_name = os.environ.get(\"VINE_MANAGER_NAME\")\n",
    "print(manager_name)\n",
    "ports_str = os.environ.get(\"VINE_MANAGER_PORTS\", \"9123, 9150\")\n",
    "ports = [int(p.strip()) for p in ports_str.split(\",\")]\n",
    "\n",
    "if len(ports) == 1:\n",
    "    ports = ports[0]\n",
    "else:\n",
    "    ports = [int(p) for p in ports]\n",
    "\n",
    "print(f\"Manager Ports: {ports}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0daebfc8-be1c-46f9-ba58-6b3e66df9634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create TaskVine manager for distributed computing\n",
    "m = DaskVine(\n",
    "    ports,\n",
    "    name=manager_name,\n",
    "    run_info_path=RUN_INFO_PATH,\n",
    "    staging_path=STAGING_PATH,\n",
    ")\n",
    "\n",
    "# Configure TaskVine settings\n",
    "m.tune(\"max-workers\", MAX_WORKERS)\n",
    "m.tune(\"max-retrievals\", MAX_RETRIEVALS)\n",
    "m.tune(\"transient-error-interval\", 1)\n",
    "m.tune(\"worker-source-max-transfers\", 10000)\n",
    "m.tune(\"transfer-temps-recovery\", 0)\n",
    "m.tune(\"attempt-schedule-depth\", 100)\n",
    "m.tune(\"watch-library-logfiles\", 1)\n",
    "m.tune(\"temp-replica-count\", TEMP_REPLICA_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec139166-26dd-4d5d-9dce-40a86516638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskVine manager 'floability-7f0c88d5-2dea-4717-9dd8-e04cf67058f9' initialized\n",
      "Listening on ports: 9124\n",
      "Run info path: vine-run-info\n"
     ]
    }
   ],
   "source": [
    "print(f\"TaskVine manager '{m.name}' initialized\")\n",
    "print(f\"Listening on ports: {m.port}\")\n",
    "print(f\"Run info path: {RUN_INFO_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aeadfc-7f08-4357-a81f-c426bfbf603c",
   "metadata": {},
   "source": [
    "# Data Preprocessing (Optional)\n",
    "Run this cell if you need to preprocess the data files. This step:\n",
    "\n",
    "- Scans the input directory structure\n",
    "- Creates metadata for all ROOT files\n",
    "- Saves results to samples_ready.json\n",
    "\n",
    "Skip this if samples_ready.json already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbeab3f2-85b3-43eb-baed-8a66fb56c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing...\n",
      "====== Preprocessing data from /users/mislam5/floability-base-dir/floability_instance_nd-dv5-run-1_20260208_180341_124243/workflow/data\n",
      "categories = ['hgg_1', 'hgg_2', 'hgg_3', 'hgg_4', 'hgg_5']\n",
      "Preprocessing samples...\n",
      "Computing preprocessing tasks...\n",
      "Preprocessing complete!\n",
      "Preprocessing complete! Time: 23.20 minutes\n",
      "Saved to: samples_ready.json\n"
     ]
    }
   ],
   "source": [
    "if PREPROCESS_DATA:\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    samples_ready = preprocess_data(\n",
    "        SAMPLES_PATH,\n",
    "        step_size=STEP_SIZE,\n",
    "        manager=m\n",
    "    )\n",
    "    \n",
    "    # Save preprocessed samples\n",
    "    with open(SAMPLES_READY_FILE, 'w') as fout:\n",
    "        json.dump(samples_ready, fout)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"Preprocessing complete! Time: {elapsed:.2f} minutes\")\n",
    "    print(f\"Saved to: {SAMPLES_READY_FILE}\")\n",
    "else:\n",
    "    print(\"Skipping preprocessing (PREPROCESS_DATA=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762137f-feeb-479a-a5b7-cc93250c26c8",
   "metadata": {},
   "source": [
    "# Load Preprocessed Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f940201-2ed5-4091-ac79-f705ebeb7840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 sample categories\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(SAMPLES_READY_FILE):\n",
    "    print(f\"Error: {SAMPLES_READY_FILE} not found!\")\n",
    "    print(\"Please run preprocessing first (set PREPROCESS_DATA=True)\")\n",
    "    raise FileNotFoundError(SAMPLES_READY_FILE)\n",
    "\n",
    "with open(SAMPLES_READY_FILE, 'r') as fin:\n",
    "    samples_ready = json.load(fin)\n",
    "\n",
    "print(f\"Loaded {len(samples_ready)} sample categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f973ab51-638d-4c0d-b165-b0cf7aee7b09",
   "metadata": {},
   "source": [
    "# Filter and Select Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891ec49f-046e-4a9d-95b5-015e589c69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include files that exist\n",
    "filtered_samples = filter_existing_files(samples_ready)\n",
    "\n",
    "\n",
    "if not filtered_samples:\n",
    "    print(\"Error: No valid files found in any dataset.\")\n",
    "    raise ValueError(\"No valid files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "801ca3f8-da5c-4b8d-bf32-6e6bfbb8260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ALL datasets (5 total)\n",
      "\n",
      "Samples to process:\n",
      "  hgg_1: 800 files\n",
      "  hgg_2: 800 files\n",
      "  hgg_3: 800 files\n",
      "  hgg_4: 800 files\n",
      "  hgg_5: 800 files\n"
     ]
    }
   ],
   "source": [
    "# Select which datasets to process\n",
    "if PROCESS_ALL_DATASETS:\n",
    "    samples_to_process = filtered_samples\n",
    "    print(f\"Processing ALL datasets ({len(samples_to_process)} total)\")\n",
    "else:\n",
    "    if SUB_DATASET not in filtered_samples:\n",
    "        print(f\"Error: Dataset '{SUB_DATASET}' not found!\")\n",
    "        print(\"Available datasets:\")\n",
    "        for name in filtered_samples.keys():\n",
    "            print(f\"  - {name}\")\n",
    "        raise ValueError(f\"Dataset '{SUB_DATASET}' not found\")\n",
    "    \n",
    "    samples_to_process = {SUB_DATASET: filtered_samples[SUB_DATASET]}\n",
    "    print(f\"Processing dataset: {SUB_DATASET}\")\n",
    "\n",
    "# Show file counts\n",
    "print(\"\\nSamples to process:\")\n",
    "for name, item in samples_to_process.items():\n",
    "    print(f\"  {name}: {len(item['files'])} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1cae88-4899-42db-9531-deed6533448b",
   "metadata": {},
   "source": [
    "# Create Analysis Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d1764d0-da89-469d-a2df-9c18955b212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating analysis tasks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/mislam5/floability-base-dir/floability_instance_nd-dv5-run-1_20260208_180341_124243/current_conda_env/lib/python3.10/site-packages/coffea/nanoevents/methods/candidate.py:11: FutureWarning: In version 2024.7.0 (target date: 2024-06-30 11:59:59-05:00), this will be an error.\n",
      "To raise these warnings as errors (and get stack traces to find out where they're called), run\n",
      "    import warnings\n",
      "    warnings.filterwarnings(\"error\", module=\"coffea.*\")\n",
      "after the first `import coffea` or use `@pytest.mark.filterwarnings(\"error:::coffea.*\")` in pytest.\n",
      "Issue: coffea.nanoevents.methods.vector will be removed and replaced with scikit-hep vector. Nanoevents schemas internal to coffea will be migrated. Otherwise please consider using that package!.\n",
      "  from coffea.nanoevents.methods import vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: hgg_1\n",
      "Signal: Higgs jets\n",
      "Processing dataset: hgg_2\n",
      "Signal: Higgs jets\n",
      "Processing dataset: hgg_3\n",
      "Signal: Higgs jets\n",
      "Processing dataset: hgg_4\n",
      "Signal: Higgs jets\n",
      "Processing dataset: hgg_5\n",
      "Signal: Higgs jets\n",
      "Analysis tasks created for 5 dataset(s)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating analysis tasks...\")\n",
    "\n",
    "# Create a wrapper function with configured parameters\n",
    "def analysis_wrapper(events):\n",
    "    return analysis(\n",
    "        events,\n",
    "        ecf_upper_bound=ECF_UPPER_BOUND,\n",
    "        triggers_file=TRIGGERS_FILE\n",
    "    )\n",
    "\n",
    "# Apply analysis to all selected datasets\n",
    "tasks = dataset_tools.apply_to_fileset(\n",
    "    analysis_wrapper,\n",
    "    samples_to_process,\n",
    "    uproot_options={\"allow_read_errors_with_report\": False},\n",
    "    schemaclass=PFNanoAODSchema,\n",
    ")\n",
    "\n",
    "print(f\"Analysis tasks created for {len(samples_to_process)} dataset(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980aefa9-bee8-4ef5-9224-71ed156af80c",
   "metadata": {},
   "source": [
    "# Execute Analysis\n",
    "\n",
    "This cell runs the distributed computation. Make sure workers are connected to the TaskVine manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc899866-b595-43d0-97da-b46a78ae8f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting computation...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4387e85332864775a77ebaa9510fac1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPUTATION COMPLETE!\n",
      "Total execution time: 635.51 seconds (10.59 minutes)\n",
      "Output saved to: output/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Starting computation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute the analysis\n",
    "computed = dask.compute(\n",
    "    tasks,\n",
    "    scheduler=m.get,\n",
    "    resources_mode=None,\n",
    "    prune_depth=PRUNE_DEPTH,\n",
    "    worker_transfers=True,\n",
    "    resources={\"cores\": 1},\n",
    ")\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"COMPUTATION COMPLETE!\")\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds ({execution_time/60:.2f} minutes)\")\n",
    "print(f\"Output saved to: {OUTPUT_DIR}/\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73431328-698b-4303-a831-f95d510ff9f5",
   "metadata": {},
   "source": [
    "# Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399abba3-2648-46fe-889c-e303c2d456ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output directory: output\n",
      "\n",
      "Datasets processed:\n",
      "  hgg_1: 800 parquet files\n",
      "  hgg_2: 800 parquet files\n",
      "  hgg_3: 800 parquet files\n",
      "  hgg_4: 800 parquet files\n",
      "  hgg_5: 800 parquet files\n"
     ]
    }
   ],
   "source": [
    "# Check output directory\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "    print(\"\\nDatasets processed:\")\n",
    "    for item in os.listdir(OUTPUT_DIR):\n",
    "        item_path = os.path.join(OUTPUT_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            files = [f for f in os.listdir(item_path) if f.endswith('.parquet')]\n",
    "            print(f\"  {item}: {len(files)} parquet files\")\n",
    "else:\n",
    "    print(f\"Output directory {OUTPUT_DIR} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84c36a65-21f8-4110-b425-aa5f980fe882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__floability_execution_done__::2026-02-08 08:45:28 PM::1c54d566\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import uuid\n",
    "\n",
    "now = datetime.now(ZoneInfo(\"America/New_York\")).strftime(\"%Y-%m-%d %I:%M:%S %p\")\n",
    "exec_id = uuid.uuid4().hex[:8]\n",
    "\n",
    "print(f\"__floability_execution_done__::{now}::{exec_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
