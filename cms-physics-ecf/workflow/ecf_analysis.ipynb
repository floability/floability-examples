{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e22136d",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f68fa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "Dataset: hgg\n",
      "ECF range: n=2 to n=3\n"
     ]
    }
   ],
   "source": [
    "# ==================== ANALYSIS CONFIGURATION ====================\n",
    "\n",
    "# Data Configuration\n",
    "SAMPLES_PATH = '/users/mislam5/tmp-data/dv5-small'  # Path to input ROOT files\n",
    "TRIGGERS_FILE = 'triggers.json'  # Path to triggers configuration\n",
    "SAMPLES_READY_FILE = 'samples_ready.json'  # Preprocessed samples metadata\n",
    "\n",
    "# Dataset Selection\n",
    "PROCESS_ALL_DATASETS = False  # Set True to process all datasets, False for specific dataset\n",
    "SUB_DATASET = 'hgg'  # Which dataset to process (e.g., 'hgg', 'hbb', 'qcd', etc.)\n",
    "\n",
    "# Analysis Parameters\n",
    "ECF_UPPER_BOUND = 3  # Calculate ECFs from n=2 to n=ECF_UPPER_BOUND (max: 6)\n",
    "STEP_SIZE = 50_000  # Number of events per chunk for preprocessing\n",
    "\n",
    "# Output Configuration\n",
    "OUTPUT_DIR = 'output'  # Base directory for output parquet files\n",
    "\n",
    "# TaskVine/Dask Configuration\n",
    "# MANAGER_NAME = 'dv5-ecf-manager'  # Name for the TaskVine manager\n",
    "# PORT_RANGE = [9123, 9128]  # Port range for TaskVine manager\n",
    "RUN_INFO_PATH = 'vine-run-info'  # Directory for TaskVine logs\n",
    "STAGING_PATH = '/tmp/ecf-staging'  # Temporary staging directory\n",
    "\n",
    "# Processing Options\n",
    "PREPROCESS_DATA = True  # Set True to run preprocessing step\n",
    "SHOW_SAMPLES = False  # Set True to display available samples and exit\n",
    "\n",
    "# Advanced TaskVine Tuning (optional)\n",
    "MAX_WORKERS = 30000  # Maximum number of workers\n",
    "MAX_RETRIEVALS = 10  # Max concurrent result retrievals\n",
    "TEMP_REPLICA_COUNT = 1  # Replication count for temp files\n",
    "PRUNE_DEPTH = 0  # Task graph pruning depth\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Dataset: {SUB_DATASET if not PROCESS_ALL_DATASETS else 'ALL'}\")\n",
    "print(f\"ECF range: n=2 to n={ECF_UPPER_BOUND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c19580",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90fefe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import dask\n",
    "import dask_awkward as dak\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "\n",
    "from coffea import dataset_tools\n",
    "from coffea.nanoevents import PFNanoAODSchema\n",
    "from ndcctools.taskvine.compat import DaskVine\n",
    "\n",
    "# Import helper functions\n",
    "from ecf_helpers import (\n",
    "    preprocess_data,\n",
    "    filter_existing_files,\n",
    "    show_available_samples,\n",
    "    analysis\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", \"Found duplicate branch\")\n",
    "warnings.filterwarnings(\"ignore\", \"Missing cross-reference index for\")\n",
    "warnings.filterwarnings(\"ignore\", \"dcut\")\n",
    "warnings.filterwarnings(\"ignore\", \"Please ensure\")\n",
    "warnings.filterwarnings(\"ignore\", \"invalid value\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3af2fd",
   "metadata": {},
   "source": [
    "## Initialize TaskVine Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b9a42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floability-406dd42c-53d4-4b70-9ee5-f563cd303715\n",
      "Manager Ports: [9123, 9150]\n"
     ]
    }
   ],
   "source": [
    "#manager_name = f\"{os.environ['USER']}-makeDF_2018_mc\";\n",
    "manager_name = os.environ.get(\"VINE_MANAGER_NAME\")\n",
    "print(manager_name)\n",
    "ports_str = os.environ.get(\"VINE_MANAGER_PORTS\", \"9123, 9150\")\n",
    "ports = [int(p.strip()) for p in ports_str.split(\",\")]\n",
    "\n",
    "if len(ports) == 1:\n",
    "    ports = ports[0]\n",
    "else:\n",
    "    ports = [int(p) for p in ports]\n",
    "\n",
    "print(f\"Manager Ports: {ports}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f59a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create TaskVine manager for distributed computing\n",
    "m = DaskVine(\n",
    "    ports,\n",
    "    name=manager_name,\n",
    "    run_info_path=RUN_INFO_PATH,\n",
    "    staging_path=STAGING_PATH,\n",
    ")\n",
    "\n",
    "# Configure TaskVine settings\n",
    "m.tune(\"max-workers\", MAX_WORKERS)\n",
    "m.tune(\"max-retrievals\", MAX_RETRIEVALS)\n",
    "m.tune(\"transient-error-interval\", 1)\n",
    "m.tune(\"worker-source-max-transfers\", 10000)\n",
    "m.tune(\"transfer-temps-recovery\", 0)\n",
    "m.tune(\"attempt-schedule-depth\", 100)\n",
    "m.tune(\"watch-library-logfiles\", 1)\n",
    "m.tune(\"temp-replica-count\", TEMP_REPLICA_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b867934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskVine manager 'floability-406dd42c-53d4-4b70-9ee5-f563cd303715' initialized\n",
      "Listening on ports: 9123\n",
      "Run info path: vine-run-info\n"
     ]
    }
   ],
   "source": [
    "print(f\"TaskVine manager '{m.name}' initialized\")\n",
    "print(f\"Listening on ports: {m.port}\")\n",
    "print(f\"Run info path: {RUN_INFO_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebeb69",
   "metadata": {},
   "source": [
    "## Data Preprocessing (Optional)\n",
    "\n",
    "Run this cell if you need to preprocess the data files. This step:\n",
    "- Scans the input directory structure\n",
    "- Creates metadata for all ROOT files\n",
    "- Saves results to `samples_ready.json`\n",
    "\n",
    "Skip this if `samples_ready.json` already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b70823d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing...\n",
      "====== Preprocessing data from /users/mislam5/tmp-data/dv5-small\n",
      "categories = ['hgg_1']\n",
      "Preprocessing samples...\n",
      "Computing preprocessing tasks...\n",
      "Preprocessing complete!\n",
      "Preprocessing complete! Time: 0.21 minutes\n",
      "Saved to: samples_ready.json\n"
     ]
    }
   ],
   "source": [
    "if PREPROCESS_DATA:\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    samples_ready = preprocess_data(\n",
    "        SAMPLES_PATH,\n",
    "        step_size=STEP_SIZE,\n",
    "        manager=m\n",
    "    )\n",
    "    \n",
    "    # Save preprocessed samples\n",
    "    with open(SAMPLES_READY_FILE, 'w') as fout:\n",
    "        json.dump(samples_ready, fout)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"Preprocessing complete! Time: {elapsed:.2f} minutes\")\n",
    "    print(f\"Saved to: {SAMPLES_READY_FILE}\")\n",
    "else:\n",
    "    print(\"Skipping preprocessing (PREPROCESS_DATA=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67dd97",
   "metadata": {},
   "source": [
    "## Load Preprocessed Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcfdaae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 sample categories\n"
     ]
    }
   ],
   "source": [
    "# Load samples metadata\n",
    "if not os.path.exists(SAMPLES_READY_FILE):\n",
    "    print(f\"Error: {SAMPLES_READY_FILE} not found!\")\n",
    "    print(\"Please run preprocessing first (set PREPROCESS_DATA=True)\")\n",
    "    raise FileNotFoundError(SAMPLES_READY_FILE)\n",
    "\n",
    "with open(SAMPLES_READY_FILE, 'r') as fin:\n",
    "    samples_ready = json.load(fin)\n",
    "\n",
    "print(f\"Loaded {len(samples_ready)} sample categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef3454",
   "metadata": {},
   "source": [
    "## Show Available Samples (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cbff892",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_SAMPLES:\n",
    "    show_available_samples(samples_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81b352",
   "metadata": {},
   "source": [
    "## Filter and Select Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17cfd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hgg_1': {'files': {'/users/mislam5/tmp-data/dv5-small/hgg_1/HJ_MINLO_Pt-200ToInf_9.root': {'object_path': 'Events', 'steps': [[0, 3000]], 'num_entries': 3000, 'uuid': '6f4df38e-6f58-11ed-abce-b0cde183beef'}, '/users/mislam5/tmp-data/dv5-small/hgg_1/HJ_MINLO_Pt-200ToInf_10.root': {'object_path': 'Events', 'steps': [[0, 3000]], 'num_entries': 3000, 'uuid': '653e7d32-6f58-11ed-9a32-c8cde183beef'}, '/users/mislam5/tmp-data/dv5-small/hgg_1/HJ_MINLO_Pt-200ToInf_1.root': {'object_path': 'Events', 'steps': [[0, 3000]], 'num_entries': 3000, 'uuid': '6556187a-6f58-11ed-8669-9bcde183beef'}, '/users/mislam5/tmp-data/dv5-small/hgg_1/HJ_MINLO_Pt-200ToInf_2.root': {'object_path': 'Events', 'steps': [[0, 2969]], 'num_entries': 2969, 'uuid': '5d79fde2-6f58-11ed-9c10-c2cde183beef'}, '/users/mislam5/tmp-data/dv5-small/hgg_1/HJ_MINLO_Pt-200ToInf_3.root': {'object_path': 'Events', 'steps': [[0, 3000]], 'num_entries': 3000, 'uuid': '6a4fb34a-6f58-11ed-a1fe-b9cde183beef'}, '/users/mislam5/tmp-data/dv5-small/hgg_1/HJ_MINLO_Pt-200ToInf_4.root': {'object_path': 'Events', 'steps': [[0, 3000]], 'num_entries': 3000, 'uuid': '6c789e2a-6f58-11ed-be9e-c5cde183beef'}, '/users/mislam5/tmp-data/dv5-small/hgg_1/HJ_MINLO_Pt-200ToInf_5.root': {'object_path': 'Events', 'steps': [[0, 3000]], 'num_entries': 3000, 'uuid': '5a3d5368-6f58-11ed-a1fe-2abee183beef'}, '/users/mislam5/tmp-data/dv5-small/hgg_1/HJ_MINLO_Pt-200ToInf_6.root': {'object_path': 'Events', 'steps': [[0, 3000]], 'num_entries': 3000, 'uuid': '761ede80-6f58-11ed-be9e-2cbee183beef'}, '/users/mislam5/tmp-data/dv5-small/hgg_1/HJ_MINLO_Pt-200ToInf_7.root': {'object_path': 'Events', 'steps': [[0, 3000]], 'num_entries': 3000, 'uuid': '6473a7c4-6f58-11ed-be9e-becde183beef'}, '/users/mislam5/tmp-data/dv5-small/hgg_1/HJ_MINLO_Pt-200ToInf_8.root': {'object_path': 'Events', 'steps': [[0, 3000]], 'num_entries': 3000, 'uuid': '69e64bee-6f58-11ed-b4a7-4bbfe183beef'}}, 'form': None, 'metadata': None}}\n",
      "Processing ALL datasets (1 total)\n",
      "\n",
      "Samples to process:\n",
      "  hgg_1: 10 files\n"
     ]
    }
   ],
   "source": [
    "# Filter to only include files that exist\n",
    "filtered_samples = filter_existing_files(samples_ready)\n",
    "\n",
    "print(filtered_samples)\n",
    "\n",
    "if not filtered_samples:\n",
    "    print(\"Error: No valid files found in any dataset.\")\n",
    "    raise ValueError(\"No valid files\")\n",
    "PROCESS_ALL_DATASETS = True\n",
    "# Select which datasets to process\n",
    "if PROCESS_ALL_DATASETS:\n",
    "    samples_to_process = filtered_samples\n",
    "    print(f\"Processing ALL datasets ({len(samples_to_process)} total)\")\n",
    "else:\n",
    "    if SUB_DATASET not in filtered_samples:\n",
    "        print(f\"Error: Dataset '{SUB_DATASET}' not found!\")\n",
    "        print(\"Available datasets:\")\n",
    "        for name in filtered_samples.keys():\n",
    "            print(f\"  - {name}\")\n",
    "        raise ValueError(f\"Dataset '{SUB_DATASET}' not found\")\n",
    "    \n",
    "    samples_to_process = {SUB_DATASET: filtered_samples[SUB_DATASET]}\n",
    "    print(f\"Processing dataset: {SUB_DATASET}\")\n",
    "\n",
    "# Show file counts\n",
    "print(\"\\nSamples to process:\")\n",
    "for name, item in samples_to_process.items():\n",
    "    print(f\"  {name}: {len(item['files'])} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f36bb",
   "metadata": {},
   "source": [
    "## Create Analysis Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdf0bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating analysis tasks...\n",
      "Processing dataset: hgg_1\n",
      "Signal: Higgs jets\n",
      "Analysis tasks created for 1 dataset(s)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating analysis tasks...\")\n",
    "\n",
    "# Create a wrapper function with configured parameters\n",
    "def analysis_wrapper(events):\n",
    "    return analysis(\n",
    "        events,\n",
    "        ecf_upper_bound=ECF_UPPER_BOUND,\n",
    "        triggers_file=TRIGGERS_FILE\n",
    "    )\n",
    "\n",
    "# Apply analysis to all selected datasets\n",
    "tasks = dataset_tools.apply_to_fileset(\n",
    "    analysis_wrapper,\n",
    "    samples_to_process,\n",
    "    uproot_options={\"allow_read_errors_with_report\": False},\n",
    "    schemaclass=PFNanoAODSchema,\n",
    ")\n",
    "\n",
    "print(f\"Analysis tasks created for {len(samples_to_process)} dataset(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23206d36",
   "metadata": {},
   "source": [
    "## Execute Analysis\n",
    "\n",
    "This cell runs the distributed computation. Make sure workers are connected to the TaskVine manager.\n",
    "\n",
    "**To start workers:**\n",
    "```bash\n",
    "vine_worker -M <manager_name> --cores 8 --memory 16000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "899d2aa4-8b5d-4749-bdee-8567ec4393af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.9.1\n",
      "    latest version: 26.1.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /users/mislam5/floability-base-dir/floability_instance_20260206_195706_796139/current_conda_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda-pack\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  conda-pack         conda-forge/noarch::conda-pack-0.9.1-pyhcf101f3_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y conda-pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "970769fe-4d95-4aa0-a026-476d84d38bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.9.1\n",
      "    latest version: 26.1.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /users/mislam5/floability-base-dir/floability_instance_20260206_195706_796139/current_conda_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - threadpoolctl\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  threadpoolctl      conda-forge/noarch::threadpoolctl-3.6.0-pyhecae5ae_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y threadpoolctl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9d5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting computation...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58044e4af14d4f94b74145fdc50527e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Starting computation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute the analysis\n",
    "computed = dask.compute(\n",
    "    tasks,\n",
    "    scheduler=m.get,\n",
    "    resources_mode=None,\n",
    "    prune_depth=PRUNE_DEPTH,\n",
    "    worker_transfers=True,\n",
    "    resources={\"cores\": 1},\n",
    "    # lib_resources={'cores': 16, 'slots': 16},\n",
    "    # task_mode=\"function-calls\",\n",
    ")\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"COMPUTATION COMPLETE!\")\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds ({execution_time/60:.2f} minutes)\")\n",
    "print(f\"Output saved to: {OUTPUT_DIR}/\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8289b7a",
   "metadata": {},
   "source": [
    "## Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2498ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output directory\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "    print(\"\\nDatasets processed:\")\n",
    "    for item in os.listdir(OUTPUT_DIR):\n",
    "        item_path = os.path.join(OUTPUT_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            files = [f for f in os.listdir(item_path) if f.endswith('.parquet')]\n",
    "            print(f\"  {item}: {len(files)} parquet files\")\n",
    "else:\n",
    "    print(f\"Output directory {OUTPUT_DIR} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811daab",
   "metadata": {},
   "source": [
    "## Load and Inspect Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load results for inspection\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "dataset_name = SUB_DATASET\n",
    "output_path = os.path.join(OUTPUT_DIR, dataset_name)\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    # Read parquet files\n",
    "    result_table = pq.read_table(output_path)\n",
    "    result_df = result_table.to_pandas()\n",
    "    \n",
    "    print(f\"\\nResults for {dataset_name}:\")\n",
    "    print(f\"Shape: {result_df.shape}\")\n",
    "    print(f\"\\nColumns: {list(result_df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(result_df.head())\n",
    "else:\n",
    "    print(f\"No output found for {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec14c70",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook processed CMS NanoAOD data and calculated Energy Correlation Functions (ECFs) for jet substructure analysis.\n",
    "\n",
    "**Key outputs:**\n",
    "- ECFs calculated from n=2 to configured upper bound\n",
    "- Color ring variable\n",
    "- Jet kinematics (pT, mass, Î·)\n",
    "- ParticleNet scores\n",
    "- Gen-matching information (for MC samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
